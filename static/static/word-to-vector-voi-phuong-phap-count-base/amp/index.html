<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>Word To Vector với phương pháp Count Base</title>

    <meta name="description" content="Đi cùng với lịch sử phát triển của xử lý ngôn ngữ tự nhiên, vector hoá từ là công đoạn không thể thiếu và quan trọng. Bài viết này sẽ nói đến phương pháp Count Base, đơn giản nhưng khi được gọt giũa lại vô cùng hiệu quả." />
    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="GMO-Z.com Vietnam Lab Center Technology Blog" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Word To Vector với phương pháp Count Base" />
    <meta property="og:description" content="Đi cùng với lịch sử phát triển của xử lý ngôn ngữ tự nhiên, vector hoá từ là công đoạn không thể thiếu và quan trọng. Bài viết này sẽ nói đến phương pháp Count Base, đơn giản nhưng khi được gọt giũa lại vô cùng hiệu quả." />
    <meta property="og:url" content="https://blog.vietnamlab.vn/word-to-vector-voi-phuong-phap-count-base/" />
    <meta property="og:image" content="https://drive.google.com/uc?id&#x3D;1MVrdk78ym7n0U5veJ-csDY0x7s_3g-ca&amp;export&#x3D;download" />
    <meta property="article:published_time" content="2019-01-04T01:21:25.000Z" />
    <meta property="article:modified_time" content="2019-12-25T02:37:55.000Z" />
    <meta property="article:tag" content="NLP" />
    <meta property="article:tag" content="Count Base" />
    <meta property="article:tag" content="machine learning" />
    <meta property="article:tag" content="co-occurence matrix" />
    
    <meta property="article:publisher" content="https://www.facebook.com/vietnamlab.vn" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Word To Vector với phương pháp Count Base" />
    <meta name="twitter:description" content="Đi cùng với lịch sử phát triển của xử lý ngôn ngữ tự nhiên, vector hoá từ là công đoạn không thể thiếu và quan trọng. Bài viết này sẽ nói đến phương pháp Count Base, đơn giản nhưng khi được gọt giũa lại vô cùng hiệu quả." />
    <meta name="twitter:url" content="https://blog.vietnamlab.vn/word-to-vector-voi-phuong-phap-count-base/" />
    <meta name="twitter:image" content="https://drive.google.com/uc?id&#x3D;1N9ANDZKvw6x8BVbWZYDlMkOSCOtklc64&amp;export&#x3D;download" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="T.T.T" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="NLP, Count Base, machine learning, co-occurence matrix" />
    <meta property="og:image:width" content="1011" />
    <meta property="og:image:height" content="468" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "GMO-Z.com Vietnam Lab Center Technology Blog",
        "url": "https://blog.vietnamlab.vn/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://blog.vietnamlab.vn/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "T.T.T",
        "image": {
            "@type": "ImageObject",
            "url": "https://drive.google.com/uc?id=0B05rqFCwNCjkRXIxZVdxdFhMb28&export=download"
        },
        "url": "https://blog.vietnamlab.vn/author/thanhtt/",
        "sameAs": []
    },
    "headline": "Word To Vector với phương pháp Count Base",
    "url": "https://blog.vietnamlab.vn/word-to-vector-voi-phuong-phap-count-base/",
    "datePublished": "2019-01-04T01:21:25.000Z",
    "dateModified": "2019-12-25T02:37:55.000Z",
    "image": {
        "@type": "ImageObject",
        "url": "https://drive.google.com/uc?id=1N9ANDZKvw6x8BVbWZYDlMkOSCOtklc64&export=download",
        "width": 1011,
        "height": 468
    },
    "keywords": "NLP, Count Base, machine learning, co-occurence matrix",
    "description": "Đi cùng với lịch sử phát triển của xử lý ngôn ngữ tự nhiên, vector hoá từ là công đoạn không thể thiếu và quan trọng. Bài viết này sẽ nói đến phương pháp Count Base, đơn giản nhưng khi được gọt giũa lại vô cùng hiệu quả.",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://blog.vietnamlab.vn/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.36" />
    <link rel="alternate" type="application/rss+xml" title="GMO-Z.com Vietnam Lab Center Technology Blog" href="../../rss/index.html" />

    <style amp-custom>
    *,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: #1292EE;
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: #000;
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #dc0050;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "•";
        margin: 0 .5em;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="../../index.html">
                GMO-Z.com Vietnam Lab Center Technology Blog
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Word To Vector với phương pháp Count Base</h1>
                <section class="post-meta">
                    T.T.T -
                    <time class="post-date" datetime="2019-01-04">04 Jan 2019</time>
                </section>
            </header>
            <figure class="post-image">
                <amp-img src="https://drive.google.com/uc?id&#x3D;1N9ANDZKvw6x8BVbWZYDlMkOSCOtklc64&amp;export&#x3D;download" width="600" height="340" layout="responsive"></amp-img>
            </figure>
            <section class="post-content">

                <p>Đi cùng với lịch sử phát triển của xử lý ngôn ngữ tự nhiên, đã có rất nhiều nghiên cứu về việc vector hoá từ. Nhìn vào các nghiên cứu đó có thể thấy hầu hết các phương pháp dựa trên một idea cơ bản đó chính là:</p><blockquote><em>Ý nghĩa của 1 từ được tạo thành từ các từ xung quanh.</em></blockquote><p>Giới khoa học gọi ngắn gọn là <strong>distributional hypothesis</strong>, và dựa vào idea này, rất nhiều nghiên cứu về việc vector hoá từ được diễn ra. Thực chất bản chất của 1 từ không có ý nghĩa, mà tuỳ thuộc vào <strong>bối cảnh, đoạn văn</strong> đang nói tới mà mới mang ý nghĩa. </p><p>Ví dụ:</p><ul>
<li>I drink beer.</li>
<li>We drink wine.</li>
</ul>
<p>Có thể dễ dàng nhận thấy gần với 「<strong>drink」</strong> là đồ uống. Và xét tiếp ví dụ sau:</p><ul>
<li>I guzzle beer.</li>
<li>We guzzle wine.</li>
</ul>
<p>Ồ có thể nhận ra được từ 「<strong>drink」</strong> và 「<strong>guzzle」</strong> được sử dụng trong cùng ngữ cảnh giống nhau, hơn nữa khả năng cao 「<strong>drink」</strong> và 「<strong>guzzle」</strong> có nghĩa giống nhau.</p><p>Vậy từ đây phương pháp nào ra đời ?</p><h3 id="phngphpcountbase">Phương pháp CountBase</h3>
<p>Phần đầu mình có nhắc đến <strong>context(bối cảnh), </strong>context được nhắc đến ở đây có nghĩa là những từ lân cận với từ đang cần vector hoá. </p><p><amp-img src="https://drive.google.com/uc?id=1o5pbL1kMBL_x2KeEga9Ni3M46RCjTgeJ&amp;export=download" alt="uc?id=1o5pbL1kMBL_x2KeEga9Ni3M46RCjTgeJ&amp;export=download" width="898" height="288" layout="responsive"></amp-img></p>
<p>Như ví dụ trên giả sử window size là 2, từ đang cần vector hoá là 「<strong>goodbye」</strong> thì có 2 từ liền kề bên phải và trái được coi là context. Độ lớn của context chính là window size. Tương tự window size bằng 1 tức là phía bên phải và trái 1 từ.</p><blockquote><em>Một số tài liệu lại sử dụng window size với ý nghĩa là size của một phía. Tuy nhiên đa số được hiểu như trên.</em></blockquote><p>Từ idea ban đầu có một phương pháp dễ dàng nghĩ tới là sẽ tiến hành count những từ xung quanh. Cụ thể là tương ứng với window size sẽ count những từ thuộc context. Chính vì vậy phương pháp này được coi là một  <strong>Count Base Method. </strong>Cũng tuỳ tài liệu mà được gọi là <strong>Statistical Method.</strong></p><p>Tiếp theo mình sẽ vừa code vừa giới thiệu chi tiết phương pháp này.</p><p>Đầu tiên là chuẩn bị dữ liệu.</p><pre><code class="language-python"># coding: utf-8
import sys
sys.path.append('..')
import os
import numpy as np


def preprocess(text):
    text = text.lower()
    text = text.replace('.', ' .')
    words = text.split(' ')
    word_to_id = {}
    id_to_word = {}
    for word in words:
        if word not in word_to_id:
            new_id = len(word_to_id)
            word_to_id[word] = new_id
            id_to_word[new_id] = word
    corpus = np.array([word_to_id[w] for w in words])
    return corpus, word_to_id, id_to_word

text = 'You say goodbye and I say hello.'
corpus, word_to_id, id_to_word = preprocess(text)

print(corpus)
#[0 1 2 3 4 1 5 6]
print(id_to_word)
#{0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
</code></pre>
<p>Dễ dàng thấy được tổng số từ là 7. Tiếp theo set window size bằng 1 và đếm những từ thuộc context.</p><figure class="kg-card kg-image-card"><amp-img src="https://drive.google.com/uc?id=134wCke5_0Rt6dVRL-Jz8p9NQowu44dJD&amp;export=download" class="kg-image" alt width="836" height="226" layout="responsive"></amp-img></figure><p>Với window size bằng 1 thì bối cảnh từ 「<strong>you」</strong> chỉ có duy nhất từ 「<strong>say」</strong>. Vậy từ say mang giá trị 1, còn các từ khác  0.</p><figure class="kg-card kg-image-card"><amp-img src="https://drive.google.com/uc?id=1RtejoN_ldx8I8rFBAr7rPtRz5QNz1BTL&amp;export=download" class="kg-image" alt width="1172" height="138" layout="responsive"></amp-img></figure><p>Tương tự từ tiếp theo là từ say. Sẽ được kết quả như sau:</p><figure class="kg-card kg-image-card"><amp-img src="https://drive.google.com/uc?id=1O0GVWGLzCtjwW99cKYqAludCTthu9yVw&amp;export=download" class="kg-image" alt width="1108" height="118" layout="responsive"></amp-img></figure><p>OK, vậy là từ 「<strong>say」</strong> có thể biểu diễn bằng vector [1, 0, 1, 0, 1, 1, 0]. Và làm tương tự với các từ còn lại.</p><figure class="kg-card kg-image-card"><amp-img src="https://drive.google.com/uc?id=1ApOWRJJCCweNy-R-SsK5fi8cOgrzGogN&amp;export=download" class="kg-image" alt width="1094" height="338" layout="responsive"></amp-img></figure><p>Hình trên chính là một ma trận được gọi là <strong>co-occurence matrix. </strong>Vậy code như thế nào để ra kết quả như trên, thực tế lượng corpus rất lớn nên không thể dùng cơm để tạo ma trận được rồi. :D</p><pre><code class="language-python">def create_co_matrix(corpus, vocab_size, window_size=1):
    '''create co-occurence matrix
    :param corpus: danh sách word id
    :param vocab_size:số từ
    :param window_size: window size
    :return: co-occurence matrix
    '''
    corpus_size = len(corpus)
    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)
    for idx, word_id in enumerate(corpus):
        for i in range(1, window_size + 1):
            left_idx = idx - i
            right_idx = idx + i
            if left_idx &gt;= 0:
                left_word_id = corpus[left_idx]
                co_matrix[word_id, left_word_id] += 1
            if right_idx &lt; corpus_size:
                right_word_id = corpus[right_idx]
                co_matrix[word_id, right_word_id] += 1
    return co_matrix
    
create_co_matrix(corpus, 7, window_size=1)

# array([[0, 1, 0, 0, 0, 0, 0],
       [1, 0, 1, 0, 1, 1, 0],
       [0, 1, 0, 1, 0, 0, 0],
       [0, 0, 1, 0, 1, 0, 0],
       [0, 1, 0, 1, 0, 0, 0],
       [0, 1, 0, 0, 0, 0, 1],
       [0, 0, 0, 0, 0, 1, 0]], dtype=int32)
</code></pre>
<h3 id="mctngngca2vector">Mức độ tương đồng của 2 vector</h3>
<p>OK, phần trước chúng ta đã có thể tạo được ma trận Co-occurence. Tức là đã có vector của từng từ rồi, việc còn lại cũng rất quan trọng trong NLP đó là tính mức độ tương đồng của các vector. Tại sao nó lại mang ý nghĩa quan trọng ? Thực tế có thể thấy 1 từ sẽ có các từ khác đồng nghĩa, chính vì vậy xác định được các từ đồng nghĩa cũng hết sức quan trọng. Và độ tương đồng của 2 vector từ càng lớn thì 2 từ đó càng mang ý nghĩa giống nhau.</p><p>Và nói đến phương pháp thì cũng có thể nghĩ đến rất nhiều phương pháp như tính tích vô hướng của 2 vector,  hay tính khoản cách eculid... Tuy nhiên liên quan đến việc tính độ tương đồng của vector từ thì <strong>cosine similarity</strong> được sử dụng rất nhiều.<br />
Định nghĩa đối với 2 vector $  x =  \big(x_{1},  x_{2},  x_{3},..., x_{n}\big) $ và $  y =  \big(y_{1},  y_{2},  y_{3},..., y_{n}\big) $</p>
<p>$$ similarity \big(x, y \big) =  \frac{x * y}{ \parallel x \parallel \parallel y \parallel }  =  \frac{ x_{1}y_{1} + ... + x_{n}y_{n}}{ \sqrt{  x_{1}^{2} + ... + x_{n}^{2} } \sqrt{  y_{1}^{2} + ... + y_{n}^{2} } } $$</p>
<p>Phần tử là <strong>tích vô hướng</strong> 2 vector, phần mẫu là <strong>norm</strong> <strong>L2 </strong>của từng vector. Điểm cần chú ý ở công thức trên là việc lấy tích vô hướng sau khi regularize.</p><blockquote>Norm là cách tính biểu thị độ lớn của vector. </blockquote><p>Nhìn một cách trực quan thì <strong>cosine similarity</strong> cho biết hướng của 2 vector như thế nào. 2 vector hoàn toàn cùng hướng với nhau thì cosine similarity = 1, hoàn toàn ngược hướng thì cosine similarity = -1.</p><p>Code của công thức trên đơn giản như sau:</p><pre><code class="language-python">def cos_similarity(x, y, eps=1e-8):
    '''Tính cosine similarity
    :param x: vector
    :param y: vector
    :param eps: Tránh trường hợp chia cho 0 khi vector 0
    :return:
    '''
    nx = x / (np.sqrt(np.sum(x ** 2)) + eps) # regularize vector x
    ny = y / (np.sqrt(np.sum(y ** 2)) + eps) # regularize vector y
    return np.dot(nx, ny)
</code></pre>
<p>OK, vậy là chúng ta đã tính được độ tương đồng của 2 vector, tiếp tục với ví dụ trên thử tính cosine similarity của từ 「 <strong>i</strong>」và「<strong>you</strong>」xem sao?</p><pre><code class="language-python">
c0 = C[word_to_id['you']]  # vector của「you」
c1 = C[word_to_id['i']]  #vector của「i」
print(cos_similarity(c0, c1))

#0.7071067691154799
</code></pre>
<h3 id="tmtngngha">Tìm từ đồng nghĩa</h3>
<p>Từ việc tính được cosine similarity như trên, không phải là quá dễ dàng để tìm được những từ đồng nghĩa hay sao. Tuy nhiên chính từ ví dụ này, cũng có thể nhận thấy phương pháp <strong>Count Base</strong> có khuyết điểm. </p><pre><code class="language-python">
def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
    '''Tìm từ đồng nghĩa
    :param query: từ cần tìm
    :param word_to_id: dict để word to id
    :param id_to_word: dict để từ id to word
    :param word_matrix: co-occurence matrix
    :param top: lấy top bao nhiêu
    '''
    if query not in word_to_id:
        print('%s is not found' % query)
        return
    print('\n[query] ' + query)
    query_id = word_to_id[query]
    query_vec = word_matrix[query_id]
    vocab_size = len(id_to_word)
    similarity = np.zeros(vocab_size)
    for i in range(vocab_size):
        similarity[i] = cos_similarity(word_matrix[i], query_vec)
    count = 0
    for i in (-1 * similarity).argsort():
        if id_to_word[i] == query:
            continue
        print(' %s: %s' % (id_to_word[i], similarity[i]))
        count += 1
        if count &gt;= top:
            return
            
most_similar("you", word_to_id, id_to_word, C)

#[query] you
# goodbye: 0.7071067691154799
# i: 0.7071067691154799
# hello: 0.7071067691154799
# say: 0.0
# and: 0.0
</code></pre>
<p>Nhìn từ kết quả có thể nhận thấy giống nhất từ 「 <strong>you</strong>」có 3 từ:「<strong>goodbye</strong>」,「<strong>i</strong>」, 「<strong>hello</strong>」. </p><p>「 <strong>you</strong>」 và 「<strong>i</strong>」đúng là 2 danh từ chỉ người, cũng mang nghĩa khá tương đồng → ok đúng.</p><p>「 <strong>you</strong>」với 「<strong>goodbye</strong>」và 「<strong>hello</strong>」mà có giá trị <strong>cosine similarity lớn</strong> thì đúng là sai thật. Đương nhiên là với lượng corpus quá ít như này cũng là 1 nguyên nhân. Bạn hãy thử với lượng corpus nhiều hơn xem kết quả có được như mong đợi không nhé. </p><p>Ngoài ra nhìn vào <strong>co-occurence matrix </strong>có thể nhận thấy ma trận có số chiều rất lớn. Nếu lượng data nhiều thì xử lý sẽ rất chậm. Và đương nhiên cũng sẽ có cách cải thiện vấn đề này.</p><h3 id="likt">Lời kết</h3>
<p>Phạm vi bài viết này chỉ mới nói đến kiến thức cơ bản nhất của phương pháp <strong>Count Base</strong>, kiến thức cơ bản này mình đọc từ cuốn sách (link phía dưới) thấy hay nên đã viết lại chi tiết nhất để ai cũng có thể hiểu được và ứng dụng ngay. Và đương nhiên bài viết này không dừng lại tại đây, những bài viết tiếp theo mình sẽ nói đến các phương pháp cải tiến phương pháp này để hiệu quả hơn nữa.</p><h3 id="tiliu">Tài liệu</h3>
<p><a href="https://www.oreilly.co.jp/books/9784873118369/">https://www.oreilly.co.jp/books/9784873118369/</a></p>

            </section>

        </article>
    </main>
    <footer class="page-footer">
        <h3>GMO-Z.com Vietnam Lab Center Technology Blog</h3>
            <p>Blog chia sẻ kỹ thuật của thành viên công ty GMO-Z.com Vietnam Lab Center</p>
        <p><a href="../../index.html">Read more posts →</a></p>
        <a class="powered" href="https://ghost.org" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a>
    </footer>
    
</body>
</html>
