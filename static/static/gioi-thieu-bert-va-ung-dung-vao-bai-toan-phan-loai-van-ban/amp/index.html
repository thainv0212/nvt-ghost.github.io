<!DOCTYPE html>
<html ⚡>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1">

    <title>Giới thiệu BERT và ứng dụng vào bài toán phân loại văn bản</title>

    <link rel="canonical" href="../index.html" />
    <meta name="referrer" content="no-referrer-when-downgrade" />
    
    <meta property="og:site_name" content="GMO-Z.com Vietnam Lab Center Technology Blog" />
    <meta property="og:type" content="article" />
    <meta property="og:title" content="Giới thiệu BERT và ứng dụng vào bài toán phân loại văn bản" />
    <meta property="og:description" content="Nếu là một người quan tâm tới Deep Learning, chắc hẳn bạn đã từng nghe tới BERT - thứ được nhắc tới liên tục trong vòng 1-2 năm trở lại đây.  Vào cuối năm 2018, các nhà nghiên cứu tại Google AI Language đã công bố mã nguồn mở cho" />
    <meta property="og:url" content="https://blog.vietnamlab.vn/gioi-thieu-bert-va-ung-dung-vao-bai-toan-phan-loai-van-ban/" />
    <meta property="article:published_time" content="2020-09-17T07:11:31.000Z" />
    <meta property="article:modified_time" content="2020-09-17T07:11:31.000Z" />
    <meta property="article:tag" content="NLP" />
    <meta property="article:tag" content="BERT" />
    <meta property="article:tag" content="PyTorch" />
    <meta property="article:tag" content="machine learning" />
    
    <meta property="article:publisher" content="https://www.facebook.com/vietnamlab.vn" />
    <meta name="twitter:card" content="summary" />
    <meta name="twitter:title" content="Giới thiệu BERT và ứng dụng vào bài toán phân loại văn bản" />
    <meta name="twitter:description" content="Nếu là một người quan tâm tới Deep Learning, chắc hẳn bạn đã từng nghe tới BERT - thứ được nhắc tới liên tục trong vòng 1-2 năm trở lại đây.  Vào cuối năm 2018, các nhà nghiên cứu tại Google AI Language đã công bố mã nguồn mở cho" />
    <meta name="twitter:url" content="https://blog.vietnamlab.vn/gioi-thieu-bert-va-ung-dung-vao-bai-toan-phan-loai-van-ban/" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="N.V.T" />
    <meta name="twitter:label2" content="Filed under" />
    <meta name="twitter:data2" content="NLP, BERT, PyTorch, machine learning" />
    
    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "publisher": {
        "@type": "Organization",
        "name": "GMO-Z.com Vietnam Lab Center Technology Blog",
        "url": "https://blog.vietnamlab.vn/",
        "logo": {
            "@type": "ImageObject",
            "url": "https://blog.vietnamlab.vn/favicon.ico",
            "width": 48,
            "height": 48
        }
    },
    "author": {
        "@type": "Person",
        "name": "N.V.T",
        "url": "https://blog.vietnamlab.vn/author/thainv/",
        "sameAs": []
    },
    "headline": "Giới thiệu BERT và ứng dụng vào bài toán phân loại văn bản",
    "url": "https://blog.vietnamlab.vn/gioi-thieu-bert-va-ung-dung-vao-bai-toan-phan-loai-van-ban/",
    "datePublished": "2020-09-17T07:11:31.000Z",
    "dateModified": "2020-09-17T07:11:31.000Z",
    "keywords": "NLP, BERT, PyTorch, machine learning",
    "description": "Nếu là một người quan tâm tới Deep Learning, chắc hẳn bạn đã từng nghe tới BERT\n- thứ được nhắc tới liên tục trong vòng 1-2 năm trở lại đây.\n\nVào cuối năm 2018, các nhà nghiên cứu tại Google AI Language đã công bố mã nguồn\nmở cho một kỹ thuật mới trong Natural Language Processing (NLP), được gọi là\nBERT (Bidirectional Encoder Representations from Transformers). Với khả năng của\nmình, BERT được coi là một bước đột phá lớn và gây được tiếng vang trong cộng\nđồng Deep Learning. BERT là gì, tại sao B",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://blog.vietnamlab.vn/"
    }
}
    </script>

    <meta name="generator" content="Ghost 3.36" />
    <link rel="alternate" type="application/rss+xml" title="GMO-Z.com Vietnam Lab Center Technology Blog" href="../../rss/index.html" />

    <style amp-custom>
    *,
    *::before,
    *::after {
        box-sizing: border-box;
    }

    html {
        overflow-x: hidden;
        overflow-y: scroll;
        font-size: 62.5%;
        -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
    }

    body {
        min-height: 100vh;
        margin: 0;
        padding: 0;
        color: #3a4145;
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        font-size: 1.7rem;
        line-height: 1.55em;
        font-weight: 400;
        font-style: normal;
        background: #fff;
        scroll-behavior: smooth;
        overflow-x: hidden;
        -webkit-font-smoothing: antialiased;
        -moz-osx-font-smoothing: grayscale;
    }

    p,
    ul,
    ol,
    li,
    dl,
    dd,
    hr,
    pre,
    form,
    table,
    video,
    figure,
    figcaption,
    blockquote {
        margin: 0;
        padding: 0;
    }

    ul[class],
    ol[class] {
        padding: 0;
        list-style: none;
    }

    img {
        display: block;
        max-width: 100%;
    }

    input,
    button,
    select,
    textarea {
        font: inherit;
        -webkit-appearance: none;
    }

    fieldset {
        margin: 0;
        padding: 0;
        border: 0;
    }

    label {
        display: block;
        font-size: 0.9em;
        font-weight: 700;
    }

    hr {
        position: relative;
        display: block;
        width: 100%;
        height: 1px;
        border: 0;
        border-top: 1px solid currentcolor;
        opacity: 0.1;
    }

    ::selection {
        text-shadow: none;
        background: #cbeafb;
    }

    mark {
        background-color: #fdffb6;
    }

    small {
        font-size: 80%;
    }

    sub,
    sup {
        position: relative;
        font-size: 75%;
        line-height: 0;
        vertical-align: baseline;
    }
    sup {
        top: -0.5em;
    }
    sub {
        bottom: -0.25em;
    }

    ul li + li {
        margin-top: 0.6em;
    }

    a {
        color: #1292EE;
        text-decoration-skip-ink: auto;
    }

    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
        margin: 0;
        font-weight: 700;
        color: #121212;
        line-height: 1.4em;
    }

    h1 {
        font-size: 3.4rem;
        line-height: 1.1em;
    }

    h2 {
        font-size: 2.4rem;
        line-height: 1.2em;
    }

    h3 {
        font-size: 1.8rem;
    }

    h4 {
        font-size: 1.7rem;
    }

    h5 {
        font-size: 1.6rem;
    }

    h6 {
        font-size: 1.6rem;
    }

    amp-img {
        height: 100%;
        width: 100%;
        max-width: 100%;
        max-height: 100%;
    }

    amp-img img {
        object-fit: cover;
    }

    .page-header {
        padding: 50px 5vmin 30px;
        text-align: center;
        font-size: 2rem;
        text-transform: uppercase;
        letter-spacing: 0.5px;
    }

    .page-header a {
        color: #121212;
        font-weight: 700;
        text-decoration: none;
        font-size: 1.6rem;
        letter-spacing: -0.1px;
    }

    .post {
        max-width: 680px;
        margin: 0 auto;
    }

    .post-header {
        margin: 0 5vmin 5vmin;
        text-align: center;
    }

    .post-meta {
        margin: 1rem 0 0 0;
        text-transform: uppercase;
        color: #738a94;
        font-weight: 500;
        font-size: 1.3rem;
    }

    .post-image {
        margin: 0 0 5vmin;
    }

    .post-image img {
        display: block;
        width: 100%;
        height: auto;
    }

    .post-content {
        padding: 0 5vmin;
    }

    .post-content > * + * {
        margin-top: 1.5em;
    }

    .post-content [id]:not(:first-child) {
        margin: 2em 0 0;
    }

    .post-content > [id] + * {
        margin-top: 1rem;
    }

    .post-content [id] + .kg-card,
    .post-content blockquote + .kg-card {
        margin-top: 40px;
    }

    .post-content > ul,
    .post-content > ol,
    .post-content > dl {
        padding-left: 1.9em;
    }

    .post-content hr {
        margin-top: 40px;
    }

    .post .post-content hr + * {
        margin-top: 40px;
    }

    .post-content amp-img {
        background-color: #f8f8f8;
    }

    .post-content blockquote {
        position: relative;
        font-style: italic;
    }

    .post-content blockquote::before {
        content: "";
        position: absolute;
        left: -1.5em;
        top: 0;
        bottom: 0;
        width: 0.3rem;
        background: #000;
    }

    .post-content :not(.kg-card):not([id]) + .kg-card {
        margin-top: 40px;
    }

    .post-content .kg-card + :not(.kg-card) {
        margin-top: 40px;
    }

    .kg-card figcaption {
        padding: 1.5rem 1.5rem 0;
        text-align: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.4em;
        opacity: 0.6;
    }

    .kg-card figcaption strong {
        color: rgba(0,0,0,0.8);
    }

    .post-content :not(pre) code {
        vertical-align: middle;
        padding: 0.15em 0.4em 0.15em;
        border: #e1eaef 1px solid;
        font-weight: 400;
        font-size: 0.9em;
        line-height: 1em;
        color: #dc0050;
        background: #f0f6f9;
        border-radius: 0.25em;
    }

    .post-content > pre {
        overflow: scroll;
        padding: 16px 20px;
        color: #fff;
        background: #1F2428;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0,0,0,.1), 0 0 1px rgba(0,0,0,.4);
    }

    .kg-embed-card {
        display: flex;
        flex-direction: column;
        align-items: center;
        width: 100%;
    }

    .kg-image-card img {
        margin: auto;
    }

    .kg-gallery-card + .kg-gallery-card {
        margin-top: 0.75em;
    }

    .kg-gallery-container {
        position: relative;
    }

    .kg-gallery-row {
        display: flex;
        flex-direction: row;
        justify-content: center;
    }

    .kg-gallery-image {
        width: 100%;
        height: 100%;
    }

    .kg-gallery-row:not(:first-of-type) {
        margin: 0.75em 0 0 0;
    }

    .kg-gallery-image:not(:first-of-type) {
        margin: 0 0 0 0.75em;
    }

    .kg-bookmark-card,
    .kg-bookmark-publisher {
        position: relative;
    }

    .kg-bookmark-container,
    .kg-bookmark-container:hover {
        display: flex;
        flex-wrap: wrap;
        flex-direction: row-reverse;
        color: currentColor;
        background: rgba(255,255,255,0.6);
        font-family: -apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen,Ubuntu,Cantarell,Open Sans,Helvetica Neue,sans-serif;
        text-decoration: none;
        border-radius: 5px;
        box-shadow: 0 2px 6px -2px rgba(0, 0, 0, 0.1), 0 0 1px rgba(0, 0, 0, 0.4);
        overflow: hidden;
    }

    .kg-bookmark-content {
        flex-basis: 0;
        flex-grow: 999;
        padding: 20px;
        order: 1;
    }

    .kg-bookmark-title {
        font-weight: 600;
        font-size: 1.5rem;
        line-height: 1.3em;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        max-height: 45px;
        margin: 0.5em 0 0 0;
        font-size: 1.4rem;
        line-height: 1.55em;
        overflow: hidden;
        opacity: 0.8;
        -webkit-line-clamp: 2;
        -webkit-box-orient: vertical;
    }

    .kg-bookmark-metadata {
        margin-top: 20px;
    }

    .kg-bookmark-metadata {
        display: flex;
        align-items: center;
        font-weight: 500;
        font-size: 1.3rem;
        line-height: 1.3em;
        white-space: nowrap;
        overflow: hidden;
        text-overflow: ellipsis;
    }

    .kg-bookmark-description {
        display: -webkit-box;
        -webkit-box-orient: vertical;
        -webkit-line-clamp: 2;
        overflow: hidden;
    }

    .kg-bookmark-metadata amp-img {
        width: 18px;
        height: 18px;
        max-width: 18px;
        max-height: 18px;
        margin-right: 10px;
    }

    .kg-bookmark-thumbnail {
        display: flex;
        flex-basis: 20rem;
        flex-grow: 1;
        justify-content: flex-end;
    }

    .kg-bookmark-thumbnail amp-img {
        max-height: 200px;
    }

    .kg-bookmark-author {
        white-space: nowrap;
        text-overflow: ellipsis;
        overflow: hidden;
    }

    .kg-bookmark-publisher::before {
        content: "•";
        margin: 0 .5em;
    }

    .kg-width-full.kg-card-hascaption {
        display: grid;
        grid-template-columns: inherit;
    }

    .post-content table {
        border-collapse: collapse;
        width: 100%;
    }

    .post-content th {
        padding: 0.5em 0.8em;
        text-align: left;
        font-size: .75em;
        text-transform: uppercase;
    }

    .post-content td {
        padding: 0.4em 0.7em;
    }

    .post-content tbody tr:nth-child(2n + 1) {
        background-color: rgba(0,0,0,0.1);
        padding: 1px;
    }

    .post-content tbody tr:nth-child(2n + 2) td:last-child {
        box-shadow:
            inset 1px 0 rgba(0,0,0,0.1),
            inset -1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:nth-child(2n + 2) td {
        box-shadow: inset 1px 0 rgba(0,0,0,0.1);
    }

    .post-content tbody tr:last-child {
        border-bottom: 1px solid rgba(0,0,0,.1);
    }

    .page-footer {
        padding: 60px 5vmin;
        margin: 60px auto 0;
        text-align: center;
        background-color: #f8f8f8;
    }

    .page-footer h3 {
        margin: 0.5rem 0 0 0;
    }

    .page-footer p {
        max-width: 500px;
        margin: 1rem auto 1.5rem;
        font-size: 1.7rem;
        line-height: 1.5em;
        color: rgba(0,0,0,0.6)
    }

    .powered {
        display: inline-flex;
        align-items: center;
        margin: 30px 0 0;
        padding: 6px 9px 6px 6px;
        border: rgba(0,0,0,0.1) 1px solid;
        font-size: 12px;
        line-height: 12px;
        letter-spacing: -0.2px;
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", "Roboto", "Oxygen", "Ubuntu", "Cantarell", "Fira Sans", "Droid Sans", "Helvetica Neue", sans-serif;
        font-weight: 500;
        color: #222;
        text-decoration: none;
        background: #fff;
        border-radius: 6px;
    }

    .powered svg {
        height: 16px;
        width: 16px;
        margin: 0 6px 0 0;
    }

    @media (max-width: 600px) {
        body {
            font-size: 1.6rem;
        }
        h1 {
            font-size: 3rem;
        }

        h2 {
            font-size: 2.2rem;
        }
    }

    @media (max-width: 400px) {
        h1 {
            font-size: 2.6rem;
            line-height: 1.15em;
        }
        h2 {
            font-size: 2rem;
            line-height: 1.2em;
        }
        h3 {
            font-size: 1.7rem;
        }
    }
    </style>

    <style amp-boilerplate>body{-webkit-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-moz-animation:-amp-start 8s steps(1,end) 0s 1 normal both;-ms-animation:-amp-start 8s steps(1,end) 0s 1 normal both;animation:-amp-start 8s steps(1,end) 0s 1 normal both}@-webkit-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-moz-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-ms-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@-o-keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}@keyframes -amp-start{from{visibility:hidden}to{visibility:visible}}</style><noscript><style amp-boilerplate>body{-webkit-animation:none;-moz-animation:none;-ms-animation:none;animation:none}</style></noscript>
    <script async src="https://cdn.ampproject.org/v0.js"></script>

    

</head>

<body class="amp-template">
    <header class="page-header">
        <a href="../../index.html">
                GMO-Z.com Vietnam Lab Center Technology Blog
        </a>
    </header>

    <main class="content" role="main">
        <article class="post">

            <header class="post-header">
                <h1 class="post-title">Giới thiệu BERT và ứng dụng vào bài toán phân loại văn bản</h1>
                <section class="post-meta">
                    N.V.T -
                    <time class="post-date" datetime="2020-09-17">17 Sep 2020</time>
                </section>
            </header>
            <section class="post-content">

                <p>Nếu là một người quan tâm tới Deep Learning, chắc hẳn bạn đã từng nghe tới BERT - thứ được nhắc tới liên tục trong vòng 1-2 năm trở lại đây.</p>
<p>Vào cuối năm 2018, các nhà nghiên cứu tại Google AI Language đã công bố mã nguồn mở cho một kỹ thuật mới trong Natural Language Processing (NLP), được gọi là BERT (Bidirectional Encoder Representations from Transformers). Với khả năng của mình, BERT được coi là một bước đột phá lớn và gây được tiếng vang trong cộng đồng Deep Learning. BERT là gì, tại sao BERT lại tuyệt vời đến vậy, cách sử dụng BERT cho các bài toán NLP, tất cả sẽ được nhắc tới trong bài viết này.</p>
<h3 id="bert">BERT</h3>
<ol>
<li>
<p>BERT là gì</p>
<p>BERT (Bidirectional Encoder Representations from Transformers) là một mô hình ngôn ngữ (Language Model) được tạo ra bởi Google AI. BERT được coi như là đột phá lớn trong Machine Learning bởi vì khả năng ứng dụng của nó vào nhiều bài toán NLP khác nhau: Question Answering, Natural Language Inference,... với kết quả rất tốt.</p>
</li>
<li>
<p>Tại sao lại cần BERT</p>
<p>Một trong những thách thức lớn nhất của NLP là vấn đề dữ liệu. Trên internet có hàng tá dữ liệu, nhưng những dữ liệu đó không đồng nhất; mỗi phần của nó chỉ được dùng cho một mục đích riêng biệt, do đó khi giải quyết một bài toán cụ thể, ta cần trích ra một bộ dữ liệu thích hợp cho bài toán của mình, và kết quả là ta chỉ có một lượng rất ít dữ liệu. Nhưng có một nghịch lý là, các mô hình Deep Learning cần lượng dữ liệu rất lớn - lên tới hàng triệu - để có thể cho ra kết quả tốt. Do đó một vấn đề được đặt ra: làm thể nào để tận dụng được nguồn dữ liệu vô cùng lớn có sẵn để giải quyết bài toán của mình. Đó là tiền đề cho một kỹ thuật mới ra đời: <b>Transfer Learning</b>. Với <b>Transfer Learning</b>các mô hình (model) "chung" nhất với tập dữ liệu khổng lồ trên internet (<b>pre-training</b>) được xây dựng và có thể được "tinh chỉnh" (<b>fine-tune</b>) cho các bài toán cụ thể. Nhờ có kỹ thuật này mà kết quả cho các bài toán được cải thiện rõ rệt, không chỉ trong NLP mà còn trong các lĩnh vực khác như Computer Vision,... BERT là một trong những đại diện ưu tú nhất trong <b>Transfer Learning</b> cho NLP, nó gây tiếng vang lớn không chỉ bởi kết quả mang lại trong nhiều bài toán khác nhau, mà còn bởi vì nó hoàn toàn miễn phí, tất cả chúng ta đều có thể sử dụng BERT cho bài toán của mình.</p>
</li>
<li>
<p>Nền tảng của BERT</p>
<p>BERT sử dụng Transformer là một mô hình <b>attention</b> (attention mechanism) học mối tương quan giữa các từ (hoặc 1 phần của từ) trong một văn bản. Transformer gồm có 2 phần chính: Encoder và Decoder, encoder thực hiện đọc dữ liệu đầu vào và decoder đưa ra dự đoán. Ở đây, BERT chỉ sử dụng Encoder.</p>
<p>Khác với các mô hình directional (các mô hình chỉ đọc dữ liệu theo 1 chiều duy nhất - trái→phải, phải→ trái) đọc dữ liệu theo dạng tuần tự, Encoder đọc toàn bộ dữ liệu trong 1 lần, việc này làm cho BERT có khả năng huấn luyện dữ liệu theo cả hai chiều, qua đó mô hình có thể học được ngữ cảnh (context) của từ tốt hơn bằng cách sử dụng những từ xung quanh nó (phải&amp;trái).</p>
  <p align="center">
 <amp-img src="https://drive.google.com/uc?id=1vFCN6yoBChfSTD8N1A1QhUINRFPFGoiu" alt="Mô hình Transformer" width="700" height="474" layout="responsive"></amp-img>
 Mô hình encoder
 </p>
<p>Hình trên mô tả nguyên lý hoạt động của Encoder. Theo đó, input đầu vào là một chuỗi các token w<sub>1</sub>, w<sub>2</sub>,...được biểu diễn thành chuỗi các vector trước khi đưa vào trong mạng neural. Output của mô hình là chuỗi ccs vector có kích thước đúng bằng kích thước input. Trong khi huấn luyện mô hình, một thách thức gặp phải là các mô hình directional truyền thống gặp giới hạn khi học ngữ cảnh của từ. Để khắc phục nhược điểm của các mô hình cũ, BERT sử dụng 2 chiến lược training như sau:</p>
<ol>
<li>
<p>Masked LM (MLM)</p>
<p>Trước khi đưa vào BERT, thì 15% số từ trong chuỗi được thay thế bởi token <b>[MASK]</b>, khi đó mô hình sẽ dự đoán từ được thay thế bởi <b>[MASK]</b> với context là các từ không bị thay thế bởi <b>[MASK]</b>. Mask LM gồm các bước xử lý sau :</p>
<ul>
<li>Thêm một classification layer với input là output của Encoder.</li>
<li>Nhân các vector đầu ra với ma trận embedding để đưa chúng về không gian từ vựng (vocabulary dimensional).</li>
<li>Tính toán xác suất của mỗi từ trong tập từ vựng sử dụng hàm softmax.</li>
</ul>
<p>Hàm lỗi (loss function) của BERT chỉ tập trung vào đánh giá các từ được đánh dấu <b>[MASKED]</b> mà bỏ qua những từ còn lại, do đó mô hình hội tụ chậm hơn so với các mô hình directional, nhưng chính điều này giúp cho mô hình hiểu ngữ cảnh tốt hơn.</p>
<p>(Trên thực tế, con số 15% không phải là cố định mà có thể thay đổi theo mục đích của bài toán.)</p>
</li>
<li>
<p>Next Sentence Prediction (NSP)</p>
<p>Trong chiến lược này, thì mô hình sử dụng một cặp câu là dữ liệu đầu vào và dự đoán câu thứ 2 là câu tiếp theo của câu thứ 1 hay không. Trong quá trình huấn luyện, 50% lượng dữ liệu đầu vào là cặp câu trong đó câu thứ 2 thực sự là câu tiếp theo của câu thứ 1, 50% còn lại thì câu thứ 2 được chọn ngẫu nhiên từ tập dữ liệu. Một số nguyên tắc được đưa ra khi xử lý dữ liệu như sau:</p>
<ul>
<li>Chèn token <b>[CLS]</b> vào trước câu đầu tiên và <b>[SEP]</b> vào cuối mỗi câu.</li>
<li>Các token trong từng câu được đánh dấu là A hoặc B.</li>
<li>Chèn thêm vector embedding biểu diễn vị trí của token trong câu (chi tiết về vector embedding này có thể tìm thấy trong bài báo về Transformer).<p align="center">
<amp-img src="https://drive.google.com/uc?id=1eE5phaQY4bPLSpEEAuc9sPia5-auOsLt" alt="NSP" width="700" height="220" layout="responsive"></amp-img>
Next Sentence Prediction
</p>
</li>
</ul>
<p>Các bước xử lý trong Next Sentence Prediction:</p>
<ul>
<li>Toàn bộ câu đầu vào được đưa vào Transformer.</li>
<li>Chuyển vector output của <b>[CLS]</b> về kích thước <i>2x1</i> bằng một classification layer.</li>
<li>Tính toán xác suất <i>IsNextSequence</i> bằng softmax.</li>
</ul>
</li>
</ol>
</li>
<li>
<p>Phương pháp Fine-tuning BERT</p>
<p>Tùy vào bài toán mà ta có các phương pháp fine-tune khác nhau:</p>
<ol>
<li>Đối với bài toán Classification, ta thêm vào một <b>Classification Layer</b> với input là output của <b>Transformer</b> cho token <b>[CLS]</b>.</li>
<li>Đối với bài toán Question Answering, model nhận dữ liệu input là đoạn văn bản cùng câu hỏi và được huấn luyện để  đánh nhãn cho câu trả lời trong đoạn văn bản đó.</li>
<li>Đối với bài toán Named Entity Recognition (NER), model được huấn luyện để dự đoán nhãn cho mỗi token (tên người, tổ chức, địa danh,...).</li>
</ol>
</li>
</ol>
<h3 id="ngdngbertvophnloivnbn">Ứng dụng BERT vào phân loại văn bản</h3>
<p>Sau khi tìm hiểu về BERT, ta sẽ cùng sử dụng BERT <b>pretrained-model</b> cho bài toán phân loại văn bản (Text Classification). Xin giải thích một chút về Text Classification, Text Classification là một trong những bài toán phổ biến nhất trong NLP, giải quyết nhiều vấn đề thực tế như phân tích ngữ nghĩa, lọc spam, phân loại tin tức... Ở trong bài viết này, ta sẽ sử dụng BERT cho bài toán phân loại tin giả - Fake news detection. Dataset được sử dụng là <a href="https://www.kaggle.com/nopdev/real-and-fake-news-dataset">REAL and FAKE news dataset</a> từ <b>Kaggle</b>.</p>
<p>Ta sử dụng thư viện <a href="https://huggingface.co/transformers/pretrained_models.html">Huggingface</a> là một thư viện cho phép sử dụng các SOTA (state-of-the-art) transformer trên ngôn ngữ Python bằng framework Pytorch. Trước khi bắt tay vào viết code, ta cần cài đặt một số thư viện sau: Pytorch, torchtext, transformer, matplotlib, pandas, numpy, seaborn.<br />
Ngoài Pytorch, BERT còn được cài đặt trên nhiều framework khác như <a href="https://github.com/Separius/BERT-keras">tensorflow</a> và <a href="https://github.com/Separius/BERT-keras">keras</a>.</p>
<ol>
<li>
<p>Tiền xử lý dữ liệu<br />
Trong phần này, ta xử lý dữ liệu từ bộ <b>REAL and FAKE news dataset</b>, mục đích là tách bộ dữ liệu ban đầu thành 3 tập <i>train, validation, test</i>. Ở đây, ta tạo thêm một trường <i>titletext</i> mới bằng cách ghép các trường <i>title</i> và <i>text</i> với nhau.</p>
<pre><code class="language-python"># Libraries
import pandas as pd
from sklearn.model_selection import train_test_split

def trim_string(x):

    x = x.split(maxsplit=first_n_words)
    x = ' '.join(x[:first_n_words])

    return x
# Read raw data
df_raw = pd.read_csv(raw_data_path)

# Prepare columns
df_raw['label'] = (df_raw['label'] == 'FAKE').astype('int')
df_raw['titletext'] = df_raw['title'] + ". " + df_raw['text']
df_raw = df_raw.reindex(columns=['label', 'title', 'text', 'titletext'])

# Drop rows with empty text
df_raw.drop( df_raw[df_raw.text.str.len() &lt; 5].index, inplace=True)

# Trim text and titletext to first_n_words
df_raw['text'] = df_raw['text'].apply(trim_string)
df_raw['titletext'] = df_raw['titletext'].apply(trim_string) 

# Split according to label
df_real = df_raw[df_raw['label'] == 0]
df_fake = df_raw[df_raw['label'] == 1]

# Train-test split
df_real_full_train, df_real_test = train_test_split(df_real, train_size = train_test_ratio, random_state = 1)
df_fake_full_train, df_fake_test = train_test_split(df_fake, train_size = train_test_ratio, random_state = 1)

# Train-valid split
df_real_train, df_real_valid = train_test_split(df_real_full_train, train_size = train_valid_ratio, random_state = 1)
df_fake_train, df_fake_valid = train_test_split(df_fake_full_train, train_size = train_valid_ratio, random_state = 1)

# Concatenate splits of different labels
df_train = pd.concat([df_real_train, df_fake_train], ignore_index=True, sort=False)
df_valid = pd.concat([df_real_valid, df_fake_valid], ignore_index=True, sort=False)
df_test = pd.concat([df_real_test, df_fake_test], ignore_index=True, sort=False)

# Write preprocessed data
df_train.to_csv(destination_folder + '/train.csv', index=False)
df_valid.to_csv(destination_folder + '/valid.csv', index=False)
df_test.to_csv(destination_folder + '/test.csv', index=False)
</code></pre>
</li>
<li>
<p>Khai báo các thư viện cần thiết</p>
<pre><code class="language-python"># Libraries
import matplotlib.pyplot as plt
import pandas as pd
import torch

# Preliminaries
from torchtext.data import Field, TabularDataset, BucketIterator, Iterator

# Models
import torch.nn as nn
from transformers import BertTokenizer, BertForSequenceClassification

# Training
import torch.optim as optim

# Evaluation
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns

</code></pre>
<p>Phần quan trọng nhất ở đây là thư viện transformer, chứa các class <code>BertTokenizer</code>, <code>BertForSequenceClassification</code> để khởi tạo bộ tách từ và mô hình phân loại.</p>
</li>
<li>
<p>Chuẩn bị dữ liệu và xử lý</p>
<pre><code class="language-python">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Model parameter
MAX_SEQ_LEN = 128
PAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
UNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)

# Fields
label_field = Field(sequential=False, use_vocab=False, batch_first=True, dtype=torch.float)
text_field = Field(use_vocab=False, tokenize=tokenizer.encode, lower=False, include_lengths=False, batch_first=True,
                   fix_length=MAX_SEQ_LEN, pad_token=PAD_INDEX, unk_token=UNK_INDEX)
fields = [('label', label_field), ('title', text_field), ('text', text_field), ('titletext', text_field)]

# Tabular Dataset
train, valid, test = TabularDataset.splits(path=source_folder, train='train.csv', validation='valid.csv',
                                           test='test.csv', format='CSV', fields=fields, skip_header=True)

# Iterator
train_iter = BucketIterator(train, batch_size=16, sort_key=lambda x: len(x.text),
                            device=device, train=True, sort=True, sort_within_batch=True)
valid_iter = BucketIterator(valid, batch_size=16, sort_key=lambda x: len(x.text),
                            device=device, train=True, sort=True, sort_within_batch=True)
test_iter = Iterator(test, batch_size=16, device=device, train=False, shuffle=False, sort=False)
</code></pre>
<p>Ở đây, ta sử dụng mô hình "bert-base-uncased" của <code>BertTokenizer</code>và tạo các trường <i>Text</i> chứa nội dung bài viết và <i>Label</i> chứa nhãn. Chiều dài dữ liệu đầu vào cho BERT sẽ giới hạn ở 128 <i>token</i>.</p>
<p><i>Một điều cần lưu ý ở đây là để sử dụng BERT tokenizer với TorchText, ta cần khai báo <code>use_vocab=False </code> và <code>tokenize=tokenizer.encode</code>. Việc này sẽ giúp cho Torchtext hiểu rằng ta sẽ không xây dựng lại bộ vocabulary từ đầu.</i></p>
</li>
<li>
<p>Xây dựng model</p>
<pre><code class="language-python">class BERT(nn.Module):
    def __init__(self):
        super(BERT, self).__init__()
        options_name = 'bert-base-uncased'
        self.encoder = BertForSequenceClassification.from_pretrained(options_name)
	
    def forward(self, text, label):
        loss, text_fea = self.encoder(text, labels=label)[:2]
        return loss, text_fea
</code></pre>
<p>Source code trong bài viết sử dụng phiên bản <i>bert-base-uncased</i> của BERT, đây là bản được huấn luyện trên bộ dữ liệu tiếng Anh lower-cased (chứa 12 layer, 768-hidden, 12-heads, 110M params). Các phiên bản khác của BERT có thể tìm thấy ở tài liệu của <a href="https://huggingface.co/transformers/pretrained_models.html">Huggingface</a>.</p>
</li>
<li>
<p>Huấn luyện mô hình</p>
<p>Dưới đây là các hàm lưu các tham số của model</p>
<pre><code class="language-python"># Save and Load functions
def save_checkpoint(save_path, model, valid_loss):
    if save_path is None:
        return
    
    state_dict = {
                     'model_state_dict': model.state_dict(),
                     'valid_loss': valid_loss
                 }
    torch.save(state_dict, save_path)
    print(f'Model saved to ==&gt; {save_path}')

def load_checkpoint(load_path, model):
    if load_path is None:
        return
    
    state_dict = torch.load(load_path, map_location=device)
    print(f'Model loaded from &lt;== {load_path}')
    
    model.load_state_dict(state_dict['model_state_dict'])
    return state_dict['valid_loss']

def save_metrics(save_path, train_loss_list, valid_loss_list, global_steps_list):
    if save_path is None:
        return
    
    state_dict = {
                     'train_loss_list': train_loss_list,
                     'valid_loss_list': valid_loss_list,
                     'global_steps_list': global_steps_list
                 }
    torch.save(state_dict, save_path)
    print(f'Model saved to ==&gt; {save_path}')
   
def load_metrics(load_path):
    if load_path is None:
        return
    
    state_dict = torch.load(load_path, map_location=device)
    print(f'Model loaded from &lt;== {load_path}')
    return state_dict['train_loss_list'], state_dict['valid_loss_list'],state_dict['global_steps_list']
</code></pre>
<p>Hàm  huấn luyện mô hình:</p>
<pre><code class="language-python"># Training function
def train(model,
         optimizer,
         criterion=nn.BCELoss(),
         train_loader=train_iter,
         valid_loader=valid_iter,
         num_epochs=5,
         eval_every=len(train_iter)//2,
         file_path=destination_folder,
         best_valid_loss=float('Inf')):
    # initialize running values
    running_loss = 0.0
    valid_running_loss = 0.0
    global_step = 0
    train_loss_list = []
    valid_loss_list = []
    global_steps_list = []
    
    # training loop
    model.train()
    for epoch in range(num_epochs):
        for (labels, title, text, titletext), _ in train_loader:
            labels = labels.type(torch.LongTensor)
            labels = labels.to(device)
            titletext = titletext.type(torch.LongTensor)
            titletext = titletext.to(device)
            output = model(titletext, labels)
            loss, _ = output
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # update running values
            running_loss += loss.item()
            global_step +=1 
            
            # evaluation step
            if global_step % eval_every == 5:
                model.eval()
                with torch.no_grad():
                    # validation loop
                    for(labels, title, text, titletext), _ in valid_loader:
                        labels = labels.type(torch.LongTensor)
                        labels = labels.to(device)
                        titletext = titletext.type(torch.LongTensor)
                        titletext = titletext.to(device)
                        output = model(titletext, labels)
                        loss, _ = output
                        
                        valid_running_loss += loss.item()
                # evaluation
                average_train_loss = running_loss / eval_every
                average_valid_loss = valid_running_loss / eval_every
                train_loss_list.append(average_train_loss)
                valid_loss_list.append(average_valid_loss)
                global_steps_list.append(global_step)
                
                # reset running values
                running_loss = 0.0
                valid_running_loss = 0.0
                model.train()
                
                # print progress
                print('Epoch [{}/{}], Step [{}/{}], Train loss: {:.4f}, Valid loss: {:.4f}'
                      .format(epoch + 1, num_epochs, global_step, num_epochs * len(train_loader), average_train_loss, average_valid_loss))
                # checkpoint
                if best_valid_loss &gt; average_valid_loss:
                    best_valid_loss = average_valid_loss
                    save_checkpoint(file_path + '/' + 'model.pt', model, best_valid_loss)
                    save_metrics(file_path + '/' + 'metrics.pt', train_loss_list, valid_loss_list, global_steps_list)
	save_metrics(file_path + '/metrics.pt', train_loss_list, valid_loss_list, global_steps_list)

model = BERT().to(device)
optimizer = optim.Adam(model.parameters(), lr=2e-5)
train(model=model, optimizer=optimizer)
</code></pre>
<p>Do bài toán fake news detection là bài toán phân loại 2 lớp, ta sử dụng <code>BinaryCrossEntropy</code> làm loss function và <code>Sigmoid</code> làm activation function. Trong quá trình huấn luyện, ta đánh giá hiệu năng của mô hình trên tập validation, sau đó lưu lại model mỗi khi <i>validation loss</i> giảm  nhằm giữ lại model tốt nhất. Dưới đây là kết quả huấn luyện model.</p>
 <p align="center">
 <amp-img src="https://drive.google.com/uc?id=1ByazSUZd8Ju_5bE4dKMqCl_5Kz_0E_qO" alt="Training progress" width="432" height="288" layout="responsive"></amp-img>
 Quá trình huấn luyện model
 </p>   
 <p align="center">
 <amp-img src="https://drive.google.com/uc?id=1bkIz2JKpIw6TTA6zj1oPeh8KgM7M_cVj" alt="Evaluation" width="536" height="454" layout="responsive"></amp-img>
 Kết quả đánh giá cho thấy mô hình đạt accuracy 92.73%
 </p>
</li>
<li>
<p>Kết luận</p>
<p>Thực nghiệm trên cho thấy chỉ với 5 epoch model BERT được fine-tuning đã cho ra kết quả rất tốt và có thể cải thiện hơn nữa, hơn nữa việc cài đặt được thực hiện dễ dàng với thư viện <a href="https://huggingface.co/transformers/pretrained_models.html">Huggingface</a>. Điều này càng cho thấy khả năng ứng dụng rất lớn của BERT trong các bài toán NLP khác. Source code trong bài viết này có thể được tải về tại <a href="https://github.com/thainv0212/bert-tutorial">đây</a>.</p>
</li>
</ol>
<h3 id="ktlun">Kết luận</h3>
<p>BERT thực sự là một bước đột phá lớn của <b>Machine Learning</b> trong lĩnh vực <b>Natural Language Processing</b>. Với <b>Transfer Learning</b>, ta hoàn toàn có thể thực hiện fine-tune mô hình có sẵn của BERT để giải quyết nhiều bài toán khác nhau trong lĩnh vực này. Trong bài viết này, tôi không đi quá sâu về kỹ thuật bên trong BERT mà chỉ trình bày những ý tưởng cơ bản của nó. Tuy nhiên, bạn đọc muốn tìm hiểu sâu hơn hoàn toàn có thể tham khảo trong tài liệu đầy của của BERT, <a href="https://arxiv.org/abs/1810.04805">paper</a> và <a href="https://github.com/google-research/bert">source code</a>. Qua bài viết này, tôi hy vọng giúp các bạn hiểu được ý tưởng của BERT và cách sử dụng BERT cho một bài toán cụ thể, qua đó có thể đưa ra một gợi ý nho nhỏ về hướng đi cho các bạn khi giải quyết một bài toán NLP trong thực tế. Nếu các bạn có góp ý về bài viết hay vấn đề cần thảo luận, xin vui lòng comment phía dưới, tôi sẽ cố gắng trả lời trong thời gian sớm nhất. Xin cảm ơn!</p>
<h3 id="tiliuthamkho">Tài liệu tham khảo</h3>
<ul>
<li><a href="https://arxiv.org/abs/1810.04805">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li>
<li><a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing</a></li>
<li><a href="https://towardsdatascience.com/bert-text-classification-using-pytorch-723dfb8b6b5b">BERT Text Classification Using Pytorch</a></li>
<li><a href="https://huggingface.co/transformers/pretrained_models.html">Huggingface</a></li>
</ul>


            </section>

        </article>
    </main>
    <footer class="page-footer">
        <h3>GMO-Z.com Vietnam Lab Center Technology Blog</h3>
            <p>Blog chia sẻ kỹ thuật của thành viên công ty GMO-Z.com Vietnam Lab Center</p>
        <p><a href="../../index.html">Read more posts →</a></p>
        <a class="powered" href="https://ghost.org" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 156 156"><g fill="none" fill-rule="evenodd"><rect fill="#15212B" width="156" height="156" rx="27"/><g transform="translate(36 36)" fill="#F6F8FA"><path d="M0 71.007A4.004 4.004 0 014 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0130 84H4a4 4 0 01-4-4.007v-8.986zM50 71.007A4.004 4.004 0 0154 67h26a4 4 0 014 4.007v8.986A4.004 4.004 0 0180 84H54a4 4 0 01-4-4.007v-8.986z"/><rect y="34" width="84" height="17" rx="4"/><path d="M0 4.007A4.007 4.007 0 014.007 0h41.986A4.003 4.003 0 0150 4.007v8.986A4.007 4.007 0 0145.993 17H4.007A4.003 4.003 0 010 12.993V4.007z"/><rect x="67" width="17" height="17" rx="4"/></g></g></svg> Published with Ghost</a>
    </footer>
    
</body>
</html>
